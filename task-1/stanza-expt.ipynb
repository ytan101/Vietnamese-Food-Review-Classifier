{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4c2b6904cc417b84952277e74659c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 09:54:57 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-01-10 09:54:58 INFO: File exists: C:\\Users\\Yixian\\stanza_resources\\en\\default.zip\n",
      "2023-01-10 09:55:01 INFO: Finished downloading models and saved to C:\\Users\\Yixian\\stanza_resources.\n",
      "2023-01-10 09:55:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bef54e3cb9340b1a801991a8a4662ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 09:55:01 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-01-10 09:55:02 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2023-01-10 09:55:02 INFO: Use device: gpu\n",
      "2023-01-10 09:55:02 INFO: Loading: tokenize\n",
      "2023-01-10 09:55:02 INFO: Loading: pos\n",
      "2023-01-10 09:55:02 INFO: Loading: lemma\n",
      "2023-01-10 09:55:02 INFO: Loading: depparse\n",
      "2023-01-10 09:55:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# proxies = {'http': 'http://ip:port', 'https': 'http://ip:port'}\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tupos: PRON\tword: My\thead id: 3\thead: dog\tdeprel: nmod:poss\n",
      "id: 2\tupos: NOUN\tword: fat\thead id: 3\thead: dog\tdeprel: compound\n",
      "id: 3\tupos: NOUN\tword: dog\thead id: 4\thead: vocalizes\tdeprel: nsubj\n",
      "id: 4\tupos: VERB\tword: vocalizes\thead id: 0\thead: root\tdeprel: root\n",
      "id: 5\tupos: ADV\tword: well\thead id: 4\thead: vocalizes\tdeprel: advmod\n",
      "id: 6\tupos: ADP\tword: into\thead id: 10\thead: microphone\tdeprel: case\n",
      "id: 7\tupos: PRON\tword: my\thead id: 10\thead: microphone\tdeprel: nmod:poss\n",
      "id: 8\tupos: ADJ\tword: new\thead id: 10\thead: microphone\tdeprel: amod\n",
      "id: 9\tupos: ADJ\tword: pink\thead id: 10\thead: microphone\tdeprel: amod\n",
      "id: 10\tupos: NOUN\tword: microphone\thead id: 4\thead: vocalizes\tdeprel: obl\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"My fat dog vocalizes well into my new pink microphone\"]\n",
    "\n",
    "def print_dep(sentences):\n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        print(*[f'id: {word.id}\\tupos: {word.upos}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
    "\n",
    "print_dep(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'pink'] ([{'ADJ': 'new', 'NOUN': 'microphone', 'dependency': 'amod'}, {'ADJ': 'pink', 'NOUN': 'microphone', 'dependency': 'amod'}], ['fat', 'dog', 'microphone'])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"My fat dog vocalizes well into my new pink microphone\"]\n",
    "\n",
    "def find_adjectives(sentence):\n",
    "    adjectives = []\n",
    "    doc = nlp(sentence)\n",
    "    adjectives = [word.text for sentence in doc.sentences for word in sentence.words if word.upos == \"ADJ\"]\n",
    "    return adjectives\n",
    "\n",
    "def find_triplets(sentence, adjectives=None):\n",
    "\n",
    "    triplets = []\n",
    "    nouns = []\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            head_word_text = sentence.words[word.head-1].text\n",
    "            if word.upos == \"NOUN\" and head_word_text in adjectives:\n",
    "                # Append in the form: (adjective, noun, deprel)\n",
    "                triplets.append({\"ADJ\": head_word_text, \"NOUN\": word.text, \"dependency\": word.deprel})\n",
    "                nouns.append(word.text)\n",
    "            elif word.upos == \"NOUN\":\n",
    "                nouns.append(word.text)\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            head_word_text = sentence.words[word.head-1].text\n",
    "            if word.upos == \"ADJ\" and head_word_text in nouns:\n",
    "                # Append in the form: (adjective, noun, deprel)\n",
    "                triplets.append({\"ADJ\": word.text, \"NOUN\": head_word_text, \"dependency\": word.deprel})\n",
    "\n",
    "    return triplets, nouns\n",
    "\n",
    "for sentence in sentences:\n",
    "    adjectives = find_adjectives(sentence)\n",
    "    triplets = find_triplets(sentence, adjectives)\n",
    "    print(adjectives, triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find adjectives\n",
    "# From noun, find head (if adj is after)\n",
    "# From adj, find head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"The\",\n",
       "      \"lemma\": \"the\",\n",
       "      \"upos\": \"DET\",\n",
       "      \"xpos\": \"DT\",\n",
       "      \"feats\": \"Definite=Def|PronType=Art\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"det\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 3\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"cheap\",\n",
       "      \"lemma\": \"cheap\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 3,\n",
       "      \"deprel\": \"amod\",\n",
       "      \"start_char\": 4,\n",
       "      \"end_char\": 9\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"laptop\",\n",
       "      \"lemma\": \"laptop\",\n",
       "      \"upos\": \"NOUN\",\n",
       "      \"xpos\": \"NN\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"start_char\": 10,\n",
       "      \"end_char\": 16\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"is\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"AUX\",\n",
       "      \"xpos\": \"VBZ\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"cop\",\n",
       "      \"start_char\": 17,\n",
       "      \"end_char\": 19\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"very\",\n",
       "      \"lemma\": \"very\",\n",
       "      \"upos\": \"ADV\",\n",
       "      \"xpos\": \"RB\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"advmod\",\n",
       "      \"start_char\": 20,\n",
       "      \"end_char\": 24\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"slow\",\n",
       "      \"lemma\": \"slow\",\n",
       "      \"upos\": \"ADJ\",\n",
       "      \"xpos\": \"JJ\",\n",
       "      \"feats\": \"Degree=Pos\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 25,\n",
       "      \"end_char\": 29\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 29,\n",
       "      \"end_char\": 30\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting adjectives\n",
    "processed = doc.sentences[0]\n",
    "adjectives = [word.lemma for word in doc.sentences[0].words if word.upos == \"ADJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheap', 'slow']\n"
     ]
    }
   ],
   "source": [
    "print(adjectives)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 random sentences from IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3bd7d10a1b0f878f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Yixian/.cache/huggingface/datasets/csv/default-3bd7d10a1b0f878f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb16528da234adcbcb438aeb4f5d6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecd1f2c3c764d268fd8a0c9d3264624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d384274be774a91b695c04abb024f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating data split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yixian\\miniconda3\\envs\\machine-learning\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:727: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Yixian/.cache/huggingface/datasets/csv/default-3bd7d10a1b0f878f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1fc6ab70a4488e83e9fdc33b147990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"data\": \"imdb_dataset.csv\"}\n",
    "imdb_dataset = load_dataset(\"csv\", data_files = data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db469784335c47438aa643be5681393b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0354d1e266f4eacb1d5aba00a9c8273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece7a87b56834e0b82d596c7e6bdb4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "# Random sample\n",
    "imdb_sample = imdb_dataset[\"data\"].shuffle(seed=5)\n",
    "imdb_sample[\"review\"]\n",
    "\n",
    "# Convert html unicode from reviews\n",
    "imdb_dataset = imdb_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})\n",
    "\n",
    "# Remove line breaks\n",
    "imdb_dataset = imdb_dataset.map(lambda x: {\"review\": x[\"review\"].replace(\"<br />\", \" \")})\n",
    "\n",
    "# Set all to lowercase\n",
    "imdb_dataset = imdb_dataset.map(lambda x: {\"review\": x[\"review\"].lower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "# Get all sentences in the dataset\n",
    "import re\n",
    "\n",
    "for review in imdb_dataset[\"data\"][\"review\"]:\n",
    "    # Split into sentences by either fullstop, question mark or exclamation mark, but ignore ellipses\n",
    "    sentences.extend(re.split(r'(?<!\\.)(?<!\\!)(?<!\\?)[.!?]', review))\n",
    "\n",
    "# Ensure that sentences have at least 20 characters\n",
    "sentences = [sentence for sentence in sentences if len(sentence) >= 20]\n",
    "\n",
    "# Get 50 random sentences\n",
    "import random\n",
    "\n",
    "random_numbers = random.sample(range(0, len(sentences)-1), 50)\n",
    "random_sentences = [sentences[number] for number in random_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this was one of the most moving movies i have ever seen',\n",
       " ' in fact, nobody gets killed',\n",
       " \" in the end freeman offers scarlet little more than strange diversion with a 'star',not even paying for gas for scarlet's dead of night return to her unchanged life in a town the name of which freeman cares not to know\",\n",
       " ' the whole thing is evocative of a previous age and previous movies but it sweeps away the old and refreshes with a modern tale of redemption amid the tommy-gun shootouts and extortion rackets',\n",
       " ' it rips off so many other films (can you say blade',\n",
       " \" here, those who don't read their lines like cigar-store indians sound like they learned them phonetically\",\n",
       " ' it all ends well with oil being struck and \"fishtail\" going to military school',\n",
       " \"  this isn't exceptionally scary or thrilling but if you have an hour and a half to kill and/or you want to end up feeling frustrated and confused, rent this winner\",\n",
       " \" all of those things are weird, but what's really the strangest thing in this movie is the acting\",\n",
       " '  i have never left the theater feeling so ashamed and cheated in my life',\n",
       " \"78:1  sound format: stereo  emergency services struggle to cope when islamic terrorists detonate a so-called 'dirty bomb' in the middle of london\",\n",
       " ' the american wong brothers (michael and russell) were supposed to star in the film together but due to prior commitments was unavailable so another western actor steven leigh took his spot',\n",
       " ' if the story/actors/etc',\n",
       " 'i have seen romantic comedies and this is one of the easiest/worst attempts at one',\n",
       " ' we are cultured and sex-mad',\n",
       " ' liebman and selby dominate the screen and communicate the intensity of their characters without flaw',\n",
       " '.. he steals the show in several scenes',\n",
       " ' i was very disappointed',\n",
       " ' they are seen as property and investment',\n",
       " '7/10 average on imdb',\n",
       " ' the other lead, who plays the head scientist, is also fairly good, but somehow not brilliant enough to portray the huge angst that goes with the part - the immense responsibility for creation of an ultimate machine of death and destruction',\n",
       " \"  opera is one of argento's best\",\n",
       " \" heath my man, go back to monster's ball-like cameos\",\n",
       " ' but this still denotes self-concern more than anything',\n",
       " ') through the first two offerings, i imagined a camera fallen into the hands of one of those fringe kids from middle or grammar school who obsessively draw war scenes or other atrocities',\n",
       " ' the problem with some of the stunts, however, is that it appears that the film makers actually killed a few horses to get those great \"stumbling horses\" bits seen later in the film',\n",
       " ' it reminds me of a c-grade porn movie with one major difference: no porn',\n",
       " ' the longest-running gag is a fart joke, and early on the scriptwriters seem to believe that having the main character get thrown in conspicuous piles of fake animal poo automatically enlivens an otherwise uninspired rehash of the spider bite scene from \"spider-man',\n",
       " ' everybody at his high school adores howard',\n",
       " ' but it was not to be',\n",
       " ' the emphasis on side walls and distant vanishing points is greater than ever, and even in the small number of exterior scenes the sky is rarely glimpsed',\n",
       " ' perhaps the filmmaker himself, or one of his mates, has written that \"review\"',\n",
       " '. that believe in aliens, 9/11 conspiracy plots, faked moon landings, peak oil and major environmentalism what i can say is this film does push buttons, make you ask questions and ultimately just forget about it',\n",
       " ' it really helps you understand just why the movie was written how it was',\n",
       " ' when he delivers the famous line, \"it may well be that in the sight of heaven you are more worthless and less fit to live than millions like this poor man\\'s child\" he is no longer a jolly santa claus surrogate, but an avenging angel who gives scrooge a much needed verbal spanking',\n",
       " ' no continuity to the plot',\n",
       " \" poorly acted for the most part, with cardboard cutouts for characters and some particularly ludicrous situations and rather stupid dialogue, this won't be topping anyone's list of forgotten classics anytime soon\",\n",
       " ' i was particularly pleased to see s',\n",
       " \" it's a progressive film for china, i guess, but it also perpetuates myths about the femininity of gay men: much is made of chinese myths in which men take on female roles\",\n",
       " ' his chemistry with (alexondra lee), as well as any supposed sexual attention, was all languid',\n",
       " 'saving grace is a nice movie to watch in a boring afternoon,when you are looking for something different than the regular scripts and wants to have some fun',\n",
       " '  still the movie drags a little at some points',\n",
       " \" i love the re-telling of the characters' back stories which often give rise to new dimensions for us to see them in\",\n",
       " 'i think that movie can`t be a scott`s film',\n",
       " 'this is a good blueprint for a study of corporate power and the dichotomoy between required public life and the need for privacy',\n",
       " ' there was no scotland, no highlands, just a hokey- looking background to make it look that way and it turned me almost from the start',\n",
       " ' he chooses the experimentation and is placed in a large metallic cell with a bad ass criminal who also survived the electrocution',\n",
       " \" fred frith's score is organic enough that it blends everything together without interfering with it naturalistic sound\",\n",
       " 'in everything is illuminated, elijah wood plays jonathan foer, a jewish american who is looking for the woman who saved his grandfather during wwii',\n",
       " '  janeway visits tuvok']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3319592c658da5e3f94cb820cf6e0f05462b74bfcef602eaf0ba14276884bff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
